---
title: "XAI Bayesian Teaching Scientific Reports supplemental analyses"
output:
  pdf_document: default
  html_notebook: default
---

# Load packages
```{r}
library(tidyverse)
```

```{r}
library(lme4)
```

```{r}
library(feather)
```
```{r}
library(viridis)
```
```{r}
library(ggnewscale)
```


```{r}
library(gridExtra)
```
```{r}
library(grid)
```

```{r}
library(extrafont)
```

```{r}
loadfonts(device = "win", quiet = T)
```

```{r}
#fonttable()
```

```{r}
library(interactions)
```

```{r}
library(ggeffects)
```

```{r}
library(RColorBrewer)
```

```{r}
sessionInfo()
```
# Load the data

### I'll only analyse the data without the 32 trials with negative RT
```{r}
df <- read_feather("../data/xai_bt_sr_main_data_clean.feather")
```

# Data preparation

### Code if participant picked the correct response as a binary variable and whether the participant response matches the ground truth of the image

```{r}
df <- df %>% 
  mutate(correct_response = as.numeric(correct == response),
    choice_category = ifelse(correct_response == 1, target_category, other_category),
         ground_truth_response = as.numeric(choice_category == ground_truth))
```


### Verify that ground truth response is correctly encoded by cross-referencing it with the prediction type variable
```{r}
df %>% 
  group_by(prediction_type, correct_response) %>% 
  summarise(ground_truth_response_proportion = mean(ground_truth_response))
```

### Create group-level standardisation for the logarithm of response time
```{r}
df <- df %>%
  group_by(subject_id) %>% 
  mutate(z_log_rt =scale(log_rt)) %>% 
  ungroup()
```

### Prepare bins for learner ratings and resnet ratings for figures
```{r}
df <- df %>% 
  mutate(p_learner_matches_target_model = exp(logp_learner_matches_target_model),
         p_learner_bins = cut(p_learner_matches_target_model, breaks = seq(0, 1, 0.25),
                              include.lowest = T, labels = c("0-25%", "26-50%", "51-75%",
                                                             "76-100%")),
         resnet_perf_bins = cut(resnet_perf, breaks = seq(0, 1, 0.25), include.lowest = T, labels = c("0-25%", "26-50%", "51-75%",
                                                             "76-100%")))
```

### Turn the teaching condition into a factor
```{r}
df <- df %>% 
  mutate(teaching_condition = factor(teaching_condition, levels = c("random", "helpful")))
```

### SDT classifications
```{r}
df <- df %>%
  mutate(response_classification = case_when(
    (prediction_type == "true_positive") & (response == correct) ~ "hit",
    (prediction_type == "true_positive") & (response != correct) ~ "miss",
    (prediction_type != "true_positive") & (response == correct) ~ "correct_rejection",
    (prediction_type != "true_positive") & (response != correct) ~ "false_alarm"))
```


# Do explanations by example help humans predict AI classification?

### Setting text size
```{r}
text_size = 25
title_size = 40
ylab_size = 30
point_size = 3.5
line_size = 4
fat = 1.5
font_style = "Arial"
```

## Formal models for intervention effectiveness

### Function for transforming logodds to probability
```{r}
logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}
```


### Prepare the data by focusing on the conditions with full examples vs no examples
```{r}
explain_df <- df %>% 
  mutate(explain_condition = ifelse(examples_condition == "no_examples" & label_condition == "labels" & highlights_condition == "no_highlights", "control_condition",
                                    ifelse(teaching_condition == "helpful" & examples_condition == "examples" & label_condition == "labels" & highlights_condition != "no_highlights",
                                           "experimental_condition", NA))) %>% 
  filter(!is.na(explain_condition)) %>% 
  mutate(explain_condition = factor(explain_condition, levels = c("control_condition", "experimental_condition")))
explain_df %>% 
  count(explain_condition)
```
# The performance of the Bayesian Teaching model

### Is the Bayesian teacher well-calibrated?
```{r}
explain_df %>% 
  filter(explain_condition == "control_condition") %>% 
  select(resnet_perf, correct_response) %>% 
  colMeans() %>% 
  round(., 4)
```
### How is the resolution of the Bayesian teacher?


```{r}
examples_df <- df %>% 
  mutate(example_contrast = ifelse(examples_condition == "no_examples" & label_condition == "labels" & highlights_condition == "no_highlights", "control", ifelse(teaching_condition == "helpful" & examples_condition == "examples" & label_condition == "labels" & highlights_condition == "no_highlights",
                                           "examples", NA))) %>% 
  filter(!is.na(example_contrast))
```

```{r}
resnet_perf_control_model <- examples_df %>% 
  filter(example_contrast == "control") %>% 
  mutate(ai_correct = as.numeric(prediction_type == "true_positive")) %>% 
  glmer(ground_truth_response ~ resnet_perf + ai_correct + ai_correct:resnet_perf + (1|subject_id), data = ., family = "binomial", control = glmerControl(optimizer="bobyqa"))


  
summary(resnet_perf_control_model)
```

```{r}
controldf <- ggpredict(resnet_perf_control_model, terms = c("resnet_perf [all]", "ai_correct"))
#exampledf <- ggpredict(resnet_perf_example_model, terms = c("resnet_perf [all]", "ai_correct"))
```

```{r}
resnet_classification_performance_contrast <- ggplot(controldf, aes(x, predicted, colour = group)) + 
  geom_line(size = 2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, show.legend=F) +
#  geom_line(data = exampledf, aes(x, predicted, colour = group), size = 2) +
#  geom_ribbon(data = exampledf, aes(ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, show.legend=F) +
  ylim(0, 1) +
  labs(x="Category accuracy", y="Accuracy") +
  ggtitle("B") +
  coord_cartesian(xlim = c(0.15, 0.85)) +
  scale_color_manual(values = c("seagreen1", "seagreen4"), name = "", labels = c("Model error", "Model hit")) +
  scale_fill_manual(values = c("seagreen1", "seagreen4")) +
  theme_minimal() +
  theme(plot.title = element_text(size=title_size, family = font_style, vjust = 0, color = "Black", face = "plain"),
        text = element_text(size=text_size, family = font_style, color = "Black"),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, family = font_style),
         axis.text.y = element_text(family = font_style, size = 25),
        axis.title = element_text(family = font_style, face = "plain", size = 30),
        axis.title.x = element_text(family = font_style, face = "plain", size = 31, vjust = -0.5),
        plot.margin = unit(c(0,0,0.5,0), "cm"),
        legend.position = "top")
```

```{r}
resnet_classification_performance_contrast
```

### Is the Bayesian teacher well-calibrated?
```{r}
df %>% 
  count(examples_condition)
```
```{r}
df %>% 
  filter(examples_condition == "examples" & teaching_condition == "random") %>% 
  select(p_learner_matches_target_model, correct_response) %>% 
  colMeans() %>% 
  round(., 4)
```
### How is the resolution of the Bayesian teacher?

```{r}
df %>% 
  filter(examples_condition == "examples" & teaching_condition == "random") %>% 
  glmer(correct_response ~ p_learner_matches_target_model + (1|subject_id), data = ., family = "binomial", control = glmerControl(optimizer="bobyqa")) %>% 
  summary(.)
```
```{r}
df %>% 
  filter(examples_condition == "examples") %>% 
  select(p_learner_matches_target_model, correct_response) %>% 
  colMeans() %>% 
  round(., 4)
```
```{r}
table(is.na(df$correct_response))
```


### How is the resolution of the Bayesian teacher?

```{r}
teacher_predictions_1 <- df %>% 
  filter(examples_condition == "no_examples" & label_condition == "labels" & highlights_condition == "no_highlights") %>% 
  mutate(ai_correct = as.numeric(prediction_type == "true_positive"),
         subject_id = factor(subject_id)) %>% 
  glmer(correct_response ~ resnet_perf + (1|subject_id), data = ., family = "binomial", control = glmerControl(optimizer="bobyqa"))
```

```{r}
teacher_predictions_3 <- df %>% 
  filter(examples_condition == "no_examples" & label_condition == "labels" & highlights_condition == "no_highlights") %>% 
  mutate(ai_correct = as.numeric(prediction_type == "true_positive")) %>% 
  glmer(correct_response ~ resnet_perf + ai_correct + ai_correct:resnet_perf +
          (1|subject_id), data = ., family = "binomial", control = glmerControl(optimizer="bobyqa"))
```

```{r}
anova(teacher_predictions_1, teacher_predictions_3)
```

```{r}
summary(teacher_predictions_3)
```
## Bayesian Teaching figure

```{r}
mydf <- ggpredict(teacher_predictions_3, terms = c("resnet_perf [all]", "ai_correct"))
```
```{r}
resnet_classification_performance2 <- ggplot(mydf, aes(x, predicted, colour = group, fill = group)) + 
  geom_line(size = 2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, show.legend=F) +
  ggtitle("A") +
  ylim(0, 1) +
  labs(x="Category accuracy", y="Fidelity") +
  coord_cartesian(xlim = c(0.15, 0.85)) +
  scale_color_manual(values = c("seagreen1", "seagreen4"), name = "", labels = c("Model error", "Model hit")) +
  scale_fill_manual(values = c("seagreen1", "seagreen4")) +
  theme_minimal() +
  theme(plot.title = element_text(size=title_size, family = font_style, vjust = 0, color = "Black", face = "plain"),
        text = element_text(size=text_size, family = font_style, color = "Black"),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, family = font_style),
         axis.text.y = element_text(family = font_style, size = 25),
        axis.title = element_text(family = font_style, face = "plain", size = 30),
        axis.title.x = element_text(family = font_style, face = "plain", size = 31, vjust = -0.5),
        plot.margin = unit(c(0,0,0.5,0), "cm"),
        legend.position = "top")
```

```{r}
expected_student_performance_vs_resnet_performance <- ggplot(data = filter(df, examples_condition == "no_examples" & label_condition == "labels" & highlights_condition == "no_highlights"), aes(x = resnet_perf, y = p_learner_matches_target_model)) +
  geom_density_2d_filled(contour_var = "count", bins = 25) +
  scale_fill_manual(values = colorRampPalette(brewer.pal(25, 
                                                                 "BuGn"))(25)) +
  labs(y="Simulated learner performance", x="Category accuracy") +
  coord_cartesian(xlim = c(0.15, 0.85)) +
  theme_minimal() +
  ggtitle("C") +
   theme(plot.title = element_text(size=title_size, family = font_style, vjust = 0, color = "Black", face = "plain"),
        text = element_text(size=text_size, family = font_style, color = "Black"), axis.text.x = element_text(angle = 90, vjust = 1, hjust=1, family = font_style),
         axis.text.y = element_text(family = font_style),
        axis.title.x = element_text(family = font_style, face = "plain", size = 30, vjust = -0.5),
        axis.title.y = element_text(family = font_style, face = "plain", size = 30),
        legend.position = "None",
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        plot.margin = unit(c(0,0,0.5,0), "cm"))
```

```{r}
grid.arrange(resnet_classification_performance2, resnet_classification_performance_contrast, ncol=2, nrow = 1, newpage = TRUE)
dev.copy(svg, "../output/supplementary_figure_teacher_model_control_only.svg", width = 14, height = 7)
dev.off()
```

